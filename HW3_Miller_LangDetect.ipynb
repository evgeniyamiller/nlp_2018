{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определение языка (language detection)\n",
    "--------------------\n",
    "\n",
    "* **Множество случаев** — тексты на разных языках\n",
    "* **Множество классов** — языки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Первый метод: частотные слова"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Очень простой и неплохой по качеству метод. Сначала создаем частотный словарь для всех языков и берем самые частотные слова. После этого для каждого текста, который нам надо расклассифицировать, смотрим, частотных слов какого языка в нем больше - тот язык и выбираем.\n",
    "\n",
    "Метод неплохо работает на текстах длиннее 50 слов и быстро имлементируется. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве корпусов и текстов для тестирования будем использовать статьи Википедии на разных языках. Скачать Википедию можно различными способами:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Дампы википедии: https://dumps.wikimedia.org/backup-index.html\n",
    "\n",
    "* wikiextractor: http://medialab.di.unipi.it/wiki/Wikipedia_Extractor\n",
    "\n",
    "* annotated_wikiextractor: https://github.com/jodaiber/Annotated-WikiExtractor\n",
    "\n",
    "* wikipedia: https://pypi.python.org/pypi/wikipedia/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Скачаем немного википедии для тестов\n",
    "Воспользуемся пакетом *wikipedia*:\n",
    "\n",
    "`pip install wikipedia`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_texts_for_lang(lang, n=10): # функция для скачивания статей из википедии\n",
    "    wikipedia.set_lang(lang)\n",
    "    wiki_content = []\n",
    "    pages = wikipedia.random(n)\n",
    "    for page_name in pages:\n",
    "        try:\n",
    "            page = wikipedia.page(page_name)\n",
    "        except wikipedia.exceptions.WikipediaException:\n",
    "            print('Skipping page {}'.format(page_name))\n",
    "            continue\n",
    "\n",
    "        wiki_content.append('{}\\n{}'.format(page.title, page.content.replace('==', '')))\n",
    "\n",
    "    return wiki_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kk 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zhenya\\A3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file C:\\Users\\zhenya\\A3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping page Михалович\n",
      "Skipping page Хтонічний\n",
      "Skipping page 14-та лінія\n",
      "Skipping page 1279 (значення)\n",
      "Skipping page Бакеєво\n",
      "Skipping page Генріх IV\n",
      "uk 94\n",
      "Skipping page Асклепіяд\n",
      "Skipping page Кулі\n",
      "Skipping page Горка\n",
      "Skipping page Брэст (значэнні)\n",
      "be 96\n",
      "Skipping page FGF\n",
      "Skipping page Intemporel\n",
      "Skipping page Nkolnguet\n",
      "Skipping page Planète rebelle (éditeur)\n",
      "fr 96\n",
      "Skipping page Виноградов, Николай\n",
      "Skipping page Родин, Борис\n",
      "Skipping page Шаве\n",
      "Skipping page ТФБ\n",
      "Skipping page Томпсон, Дэвид\n",
      "ru 95\n"
     ]
    }
   ],
   "source": [
    "import wikipedia # скачиваем по 100 статей для каждого языка. Это может занять какое-то время (5-10 минут. как правило)\n",
    "\n",
    "wiki_texts = {}\n",
    "for lang in ('kk', 'uk', 'be', 'fr', 'ru'): # казахский в википедии — это kk,\n",
    "                                      # украинский — uk, а белорусский — be\n",
    "    wiki_texts[lang] = get_texts_for_lang(lang, 100)\n",
    "    print(lang, len(wiki_texts[lang]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IC 5173A\n",
      "IC 5173A — Үндіс шоқжұлдызында орналасқан Sc (жинақы спиральді галактика) типті галактика.\n",
      "\n",
      "\n",
      " Ашылуы \n",
      "Бұл нысан Индекс каталогтың түпнұсқа басылымының тізіміне енеді. Сондай-ақ, бұл ғарыш объектісі өзге де зерттеулер мен каталогтарда кездескендіктен оның келесідей атаулары бар: PGC 68379, ESO 76-8A, AM 2210-693, Southern Integral-sign.\n",
      "\n",
      "\n",
      " Тағы қараңыз \n",
      "Мессье нысандарының тізімі\n",
      "Жаңа жалпы каталог\n",
      "Индекс каталог\n",
      "\n",
      "\n",
      " Сыртқы сілтемелер \n",
      "«Жаңа жалпы каталог» түпнұсқасындағы ағылшынша және французша мәліметтер\n",
      "Қайта қаралған Жаңа жалпы каталогтағы мәліметтер (ағыл.)\n",
      "SIMBAD базасындағы мәліметтер (ағыл.)\n",
      "VizieR базасындағы мәліметтер (ағыл.)\n",
      "NASA/IPAC Extragalactic Database базасындағы мәліметтер (ағыл.)\n",
      "IC 5173A нысанына арналған жарияланымдар\n",
      "Col de Perty\n",
      "Le col de Perty à 1 302 mètres d'altitude relie Saint-Auban-sur-l'Ouvèze à l'ouest et Orpierre à l'est.\n",
      "\n",
      "\n",
      " Situation \n",
      "Il se situe dans le massif des Baronnies, à l'est de la montagne de la Clavelière, dans le département de la Drôme entre les communes de Montauban-sur-l'Ouvèze côté ouest et de Laborel côté est. Il se situe également juste au nord de la montagne de Chamouse (1 532 m), haut lieu de randonnée dans le parc naturel régional des Baronnies provençales.\n",
      "\n",
      "\n",
      " Cyclisme \n",
      "\n",
      "\n",
      "= Profil de l'ascension =\n",
      "Sur le versant ouest, l'ascension commence à Montauban-sur-l'Ouvèze (729 m) pour 11 km d'ascension à 5,2 % de moyenne avec toujours des pourcentages très réguliers. Au hameau de Ruissas (844 m), à 8,6 km du sommet du col, une série de grands lacets débute.\n",
      "\n",
      "Le versant est part de Laborel (827 m) pour 8,5 km d'ascension à 5,6 % de moyenne avec également des pourcentages réguliers, souvent entre 5 et 6,5 % à partir de ce village.\n",
      "\n",
      "\n",
      "= Courses professionnelles =\n",
      "Le Tour de France a emprunté à cinq reprises ce col, classé 3e catégorie :\n",
      "lors de la 19e étape du Tour de France 1958 entre Carpentras et Gap, Jean Dotto est passé en tête ;\n",
      "lors de la 15e étape du Tour de France 1960 entre Avignon et Gap, Martin Van den Borgh est passé en tête ;\n",
      "lors de la 15e étape du Tour de France 1965 entre Carpentras et Gap, José Antonio Momeñe est passé en tête ;\n",
      "lors de la 12e étape du Tour de France 1972 entre Carpentras et Orcières-Merlette, Lucien Van Impe est passé en tête ;\n",
      "lors de la 14e étape du Tour de France 2006 entre Montélimar et Gap, David Canada est passé en tête.\n",
      "En 2015, les coureurs du Critérium du Dauphiné empruntent le col lors de la 4e étape Anneyron - Sisteron.\n",
      "\n",
      "\n",
      " Rallye automobile \n",
      "Le col est très régulièrement emprunté par le rallye Monte-Carlo (plus de 15 fois depuis 1973), le dernier passage en 2012 lors de l'épreuve spéciale 14.\n",
      "\n",
      "\n",
      " Notes et références \n",
      "\n",
      " Portail de la montagne\n",
      " Portail des Alpes\n",
      " Portail de la Drôme\n"
     ]
    }
   ],
   "source": [
    "print(wiki_texts['kk'][0]) # распечатаем пару текстов, чтобы убедиться, что все хорошо\n",
    "print(wiki_texts['fr'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Считаем частотный список примерно так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255\t—\n",
      "167\tжәне\n",
      "110\t\n",
      "87\tсу\n",
      "79\tбойынша\n",
      "75\tмен\n",
      "72\tдереккөздер\n",
      "60\tкоды\n",
      "59\tөзен\n",
      "54\t-\n",
      "53\tсыртқы\n",
      "53\tсілтемелер\n",
      "46\tжер\n",
      "45\tбұл\n",
      "44\tүшін\n",
      "43\tмәліметтер\n",
      "40\tжылы\n",
      "38\tоның\n",
      "38\tсаны\n",
      "37\tболып\n",
      "37\tол\n",
      "35\tадамды\n",
      "33\tқұрамына\n",
      "33\t«вконтакте»\n",
      "32\tтұрғындарының\n",
      "32\tжатқан\n",
      "30\tресей\n",
      "29\tорналасқан.\n",
      "29\tалып\n",
      "29\tаумағы\n",
      "29\tөзеннің\n",
      "28\tкм²\n",
      "27\tтағы\n",
      "27\tқазақстан\n",
      "27\tәлеуметтік\n",
      "26\t–\n",
      "26\tжайық\n",
      "25\tөзенінің\n",
      "24\tжылғы\n",
      "24\tбасқа\n",
      "23\tжалпы\n",
      "23\tресми\n",
      "21\tхалық\n",
      "21\tбір\n",
      "21\tосы\n",
      "21\tда\n",
      "21\tдейін\n",
      "21\tалабы\n",
      "21\tжыл\n",
      "20\tсол\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "import collections\n",
    "import sys\n",
    "\n",
    "def tokenize(text):\n",
    "    return text.split(' ')\n",
    "\n",
    "freqs_kk = collections.defaultdict(lambda: 0)\n",
    "\n",
    "corpus = wiki_texts['kk']\n",
    "for article in corpus:\n",
    "    for word in tokenize(article.replace('\\n', '').lower()):\n",
    "        freqs_kk[word] += 1\n",
    "\n",
    "for word in sorted(freqs_kk, key=lambda w: freqs_kk[w], reverse=True)[:50]:\n",
    "    print('{}\\t{}'.format(freqs_kk[word], word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "langs = ('kk', 'uk', 'be', 'fr', 'ru')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq_dict(langs, wiki_texts):\n",
    "    freqs = {}\n",
    "    for lang in langs:\n",
    "        freqsl = collections.defaultdict(lambda: 0)\n",
    "        corpus = wiki_texts[lang]\n",
    "        for article in corpus:\n",
    "            for word in tokenize(article.replace('\\n', '').lower()):\n",
    "                freqsl[word] += 1\n",
    "        \n",
    "        top50 = []\n",
    "        for ngram in sorted(freqsl, key=lambda n: freqsl[n], reverse=True)[:50]:\n",
    "            top50.append(ngram)\n",
    "         \n",
    "        freqs[lang] = top50\n",
    "        \n",
    "    return freqs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "freqs = freq_dict(langs, wiki_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Нужно сделать это для каждого языка и отфильтровать повторяющиеся"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь можно загружать готовые частотные словари и классифицировать тексты, просто считая количество слов в каждом."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Второй метод: частотные символьные n-граммы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим функцию, которая преобразовывает строку в массив n-грамм заданной длины."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice, tee\n",
    "\n",
    "def make_ngrams(text):\n",
    "    N = 3 # задаем длину n-граммы\n",
    "    ngrams = zip(*(islice(seq, index, None) for index, seq in enumerate(tee(text, N))))\n",
    "    ngrams = [''.join(x) for x in ngrams]\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь создадим частотные словари n-грамм аналогично первому методу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7961\t de\n",
      "7464\tes \n",
      "6212\tde \n",
      "3927\te d\n",
      "3826\tle \n",
      "3730\t le\n",
      "3332\tnt \n",
      "3253\tent\n",
      "3252\t la\n",
      "3207\tla \n",
      "3025\te l\n",
      "2646\tre \n",
      "2594\ts d\n",
      "2506\tet \n",
      "2443\t co\n",
      "2435\tion\n",
      "2429\tque\n",
      "2252\ton \n",
      "2219\t et\n",
      "2197\tles\n",
      "2044\tne \n",
      "2032\ten \n",
      "1950\t en\n",
      "1904\t pa\n",
      "1884\ttio\n",
      "1841\te p\n",
      "1840\tt d\n",
      "1838\tdes\n",
      "1824\tns \n",
      "1793\te s\n",
      "1736\tue \n",
      "1714\t l'\n",
      "1713\te c\n",
      "1706\tur \n",
      "1655\tpar\n",
      "1571\t à \n",
      "1547\te, \n",
      "1547\t qu\n",
      "1528\test\n",
      "1515\tmen\n",
      "1509\t pr\n",
      "1509\t po\n",
      "1502\tant\n",
      "1502\t un\n",
      "1455\te e\n",
      "1447\tte \n",
      "1424\tati\n",
      "1379\tlle\n",
      "1347\t du\n",
      "1311\tiqu\n"
     ]
    }
   ],
   "source": [
    "freqs_fr2 = collections.defaultdict(lambda: 0)\n",
    "\n",
    "corpus = wiki_texts['fr']\n",
    "for article in corpus:\n",
    "    for ngram in make_ngrams(article.replace('\\n', '').lower()):\n",
    "       # ngram = ngram.replace(' ', '')\n",
    "        freqs_fr2[ngram] += 1\n",
    "\n",
    "for ngram in sorted(freqs_fr2, key=lambda n: freqs_fr2[n], reverse=True)[:50]:\n",
    "    print('{}\\t{}'.format(freqs_fr2[ngram], ngram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq_ngr(langs, wiki_texts):\n",
    "    freqs_ngram = {}\n",
    "    for lang in langs:\n",
    "        freqs = collections.defaultdict(lambda: 0)\n",
    "        corpus = wiki_texts[lang]\n",
    "        for article in corpus:\n",
    "            for ngram in make_ngrams(article.replace('\\n', '').lower()):\n",
    "     #   ngram = ngram.replace(' ', '')\n",
    "                freqs[ngram] += 1\n",
    "        \n",
    "        top50 = []\n",
    "        for ngram in sorted(freqs, key=lambda n: freqs[n], reverse=True)[:50]:\n",
    "            top50.append(ngram)\n",
    "         \n",
    "        freqs_ngram[lang] = top50\n",
    "        \n",
    "    return freqs_ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs_ngram = freq_ngr(langs, wiki_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' de', 'es ', 'de ', 'e d', 'le ', ' le', 'nt ', 'ent', ' la', 'la ', 'e l', 're ', 's d', 'et ', ' co', 'ion', 'que', 'on ', ' et', 'les', 'ne ', 'en ', ' en', ' pa', 'tio', 'e p', 't d', 'des', 'ns ', 'e s', 'ue ', \" l'\", 'e c', 'ur ', 'par', ' à ', 'e, ', ' qu', 'est', 'men', ' pr', ' po', 'ant', ' un', 'e e', 'te ', 'ati', 'lle', ' du', 'iqu']\n"
     ]
    }
   ],
   "source": [
    "print(freqs_ngram['fr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Нужно сделать это для каждого языка и отфильтровать повторяющиеся"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь, как и в предыдущем методе, можно загружать готовые частотные словари n-грамм и классифицировать тексты, просто подсчитывая частотные n-граммы в каждом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_language_ngram(text, freqs_ngram):\n",
    "    text_ngrams = make_ngrams(text.lower())\n",
    "    \n",
    "    lang2sim = {}\n",
    "    \n",
    "    for lang in ('kk', 'uk', 'be', 'fr', 'ru'):\n",
    "        intersect = len(set(text_ngrams) & set(freqs_ngram[lang]))\n",
    "        lang2sim[lang] = intersect\n",
    "    \n",
    "    return max(lang2sim, key=lambda x: lang2sim[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_language_dict(text, freqs):\n",
    "    text_words = tokenize(text.replace('\\n', '').lower())\n",
    "    \n",
    "    lang2sim = {}\n",
    "    \n",
    "    for lang in ('kk', 'uk', 'be', 'fr', 'ru'):\n",
    "        intersect = len(set(text_words) & set(freqs[lang]))\n",
    "        lang2sim[lang] = intersect\n",
    "    \n",
    "    return max(lang2sim, key=lambda x: lang2sim[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = predict_language(wiki_texts['kk'][0], freqs_ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kk'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "for lang in langs:\n",
    "    for text in wiki_texts[lang]:\n",
    "        true_labels.append(lang)\n",
    "        predicted_labels.append(predict_language_ngram(text, freqs_ngram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         be       1.00      0.98      0.99        96\n",
      "         fr       1.00      1.00      1.00        96\n",
      "         kk       1.00      1.00      1.00       100\n",
      "         ru       1.00      1.00      1.00        95\n",
      "         uk       0.98      1.00      0.99        94\n",
      "\n",
      "avg / total       1.00      1.00      1.00       481\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(true_labels, predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "for lang in langs:\n",
    "    for text in wiki_texts[lang]:\n",
    "        true_labels.append(lang)\n",
    "        predicted_labels.append(predict_language_dict(text, freqs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         be       1.00      0.99      0.99        96\n",
      "         fr       1.00      1.00      1.00        96\n",
      "         kk       1.00      1.00      1.00       100\n",
      "         ru       1.00      0.98      0.99        95\n",
      "         uk       0.97      1.00      0.98        94\n",
      "\n",
      "avg / total       0.99      0.99      0.99       481\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(true_labels, predicted_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оба метода, и по словам, и по n-граммам, работают очень хорошо, но n-граммы немного лучше."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим на новых текстах:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kk 5\n",
      "uk 5\n",
      "be 5\n",
      "fr 5\n",
      "ru 5\n"
     ]
    }
   ],
   "source": [
    "wiki_texts = {}\n",
    "for lang in ('kk', 'uk', 'be', 'fr', 'ru'): # казахский в википедии — это kk,\n",
    "                                      # украинский — uk, а белорусский — be\n",
    "    wiki_texts[lang] = get_texts_for_lang(lang, 5)\n",
    "    print(lang, len(wiki_texts[lang]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "for lang in langs:\n",
    "    for text in wiki_texts[lang]:\n",
    "        true_labels.append(lang)\n",
    "        predicted_labels.append(predict_language_ngram(text, freqs_ngram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         be       1.00      1.00      1.00         5\n",
      "         fr       1.00      1.00      1.00         5\n",
      "         kk       1.00      1.00      1.00         5\n",
      "         ru       1.00      1.00      1.00         5\n",
      "         uk       1.00      1.00      1.00         5\n",
      "\n",
      "avg / total       1.00      1.00      1.00        25\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(true_labels, predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "for lang in langs:\n",
    "    for text in wiki_texts[lang]:\n",
    "        true_labels.append(lang)\n",
    "        predicted_labels.append(predict_language_dict(text, freqs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         be       1.00      1.00      1.00         5\n",
      "         fr       1.00      1.00      1.00         5\n",
      "         kk       1.00      1.00      1.00         5\n",
      "         ru       1.00      1.00      1.00         5\n",
      "         uk       1.00      1.00      1.00         5\n",
      "\n",
      "avg / total       1.00      1.00      1.00        25\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(true_labels, predicted_labels))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
